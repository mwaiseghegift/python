{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"  \n",
    "Neural Network: Implement approximated TD learning algorithm of your choice (Q-learning or\n",
    "SARSA) using neural networks from the keras package and apply this algorithm to solve the\n",
    "environments. Compute the models ùëÑ(ùë†, ùëé, ùë§ ùëÑ ) & ùúã(ùë†, ùë§ ùúã )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/retech/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation=relu))\n",
    "        model.add(Dense(24, activation=relu))\n",
    "        model.add(Dense(self.action_size, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1,\n",
    "                            verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def get_reward(state, action):\n",
    "    state = np.reshape(state, [1, 2])\n",
    "    action = np.reshape(action, [1, 1])\n",
    "    reward = model.predict(state)\n",
    "    reward = reward[0][action]\n",
    "    return reward\n",
    "\n",
    "def train_dqn(model, episodes, batch_size, max_steps):\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, 2])\n",
    "        total_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            env.render()\n",
    "            action = model.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, 2])\n",
    "            total_reward += reward\n",
    "            model.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, episodes, total_reward, model.epsilon))\n",
    "                break\n",
    "            model.replay(batch_size)\n",
    "\n",
    "def random_policy(state):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyper Parameters:\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "    max_steps = 200\n",
    "    state_size = 2\n",
    "    action_size = env.action_space.n\n",
    "    model = DQN(state_size, action_size)\n",
    "    train_dqn(model, episodes, batch_size, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/retech/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "# Add the first layer\n",
    "model.add(Dense(24, input_dim=2, activation='relu'))\n",
    "# Add the second layer\n",
    "model.add(Dense(24, activation='relu'))\n",
    "# Add the third layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the variables\n",
    "episodes = 1000\n",
    "max_steps = 200\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Initialize the lists\n",
    "rewards = []\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the episodes\n",
    "for e in range(episodes):\n",
    "    # Initialize the variables\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 2])\n",
    "    total_reward = 0\n",
    "    loss = 0\n",
    "    done = False\n",
    "    # Run the episode\n",
    "    for step in range(max_steps):\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        # Choose an action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(model.predict(state))\n",
    "        # Take the action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state = np.reshape(new_state, [1, 2])\n",
    "        # Update the variables\n",
    "        total_reward += reward\n",
    "        # Update the model\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + gamma * np.amax(model.predict(new_state)[0])\n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        # Update the variables\n",
    "        state = new_state\n",
    "        # Update the epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        # Update the lists\n",
    "        rewards.append(total_reward)\n",
    "        losses.append(loss)\n",
    "        # Check if the episode is done\n",
    "        if done:\n",
    "            print(\"Episode: {}/{}, Reward: {}, Epsilon: {}\".format(e, episodes, total_reward, epsilon))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b297c081ff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13c4fa65054c0d3bfb9dbdc50fbf8af2a6ced466e99fd692bb576917c08ad093"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
